{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74641863",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Импорт\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Загрузка набора данных IMDB\n",
    "dataset_path = r\"C:\\Users\\ASUS\\DataBase\\IMDB Dataset.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Выбор 20-25 отзывов из набора данных\n",
    "selected_reviews = df['review'][:25]\n",
    "\n",
    "# Вывод выбранных 25 отзывов\n",
    "# for i, review in enumerate(selected_reviews):\n",
    "#     print(f\"Отзыв {i + 1}:\\n{review}\\n{'='*50}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b70dfbab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Очистка HTML-тегов\n",
    "cleaned_reviews = [BeautifulSoup(review, \"html.parser\").get_text() for review in selected_reviews]\n",
    "\n",
    "# Вывод \n",
    "# for i, cleaned_review in enumerate(cleaned_reviews):\n",
    "#     print(f\"Очищенный отзыв {i + 1}:\\n{cleaned_review}\\n{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "054faa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мешок слов (Bag-of-Words) для первых 5 отзывов:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 2]\n",
      " [0 0 0 ... 1 0 0]]\n",
      "\n",
      "Имена признаков (слов):\n",
      "['12' '15' '1963' ... 'york' 'young' 'zombie']\n"
     ]
    }
   ],
   "source": [
    "# Использование CountVectorizer для создания представления \"мешка слов\"\n",
    "vectorizer = CountVectorizer(max_features=10000, max_df=0.15)\n",
    "X = vectorizer.fit_transform(cleaned_reviews)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Выводим представление \"мешка слов\"\n",
    "print(\"Мешок слов (Bag-of-Words) для первых 5 отзывов:\")\n",
    "print(X[:5].toarray())  # Выводим только первые 5 отзывов для простоты\n",
    "\n",
    "# Выводим имена признаков\n",
    "print(\"\\nИмена признаков (слов):\")\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0236fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Имена признаков (слов) с учетом стоп-слов:\n",
      "['12' '15' '1963' ... 'york' 'young' 'zombie']\n"
     ]
    }
   ],
   "source": [
    "# Использование стоп-слов на английском языке из scikit-learn\n",
    "stop_words = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Использование CountVectorizer с учетом стоп-слов\n",
    "vectorizer_with_stopwords = CountVectorizer(max_features=10000, max_df=0.15, stop_words=stop_words)\n",
    "X_with_stopwords = vectorizer_with_stopwords.fit_transform(cleaned_reviews)\n",
    "feature_names_with_stopwords = vectorizer_with_stopwords.get_feature_names_out()\n",
    "\n",
    "# Выводим имена признаков (слов) с учетом стоп-слов\n",
    "print(\"\\nИмена признаков (слов) с учетом стоп-слов:\")\n",
    "print(feature_names_with_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "749212ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF представление (без учета стоп-слов) для первых 5 отзывов:\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.09297697 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.22818663]\n",
      " [0.         0.         0.         ... 0.08450363 0.         0.        ]]\n",
      "\n",
      "TF-IDF представление (с учетом стоп-слов) для первых 5 отзывов:\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.09929979 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.22915658]\n",
      " [0.         0.         0.         ... 0.09441266 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Преобразование мешка слов в TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X)\n",
    "X_tfidf_with_stopwords = tfidf_transformer.fit_transform(X_with_stopwords)\n",
    "\n",
    "# Вывести TF-IDF представление для первых 5 отзывов без учета стоп-слов\n",
    "print(\"TF-IDF представление (без учета стоп-слов) для первых 5 отзывов:\")\n",
    "print(X_tfidf[:5].toarray())\n",
    "\n",
    "# Вывести TF-IDF представление для первых 5 отзывов с учетом стоп-слов\n",
    "print(\"\\nTF-IDF представление (с учетом стоп-слов) для первых 5 отзывов:\")\n",
    "print(X_tfidf_with_stopwords[:5].toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6211bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание целевой переменной (sentiment)\n",
    "y = df['sentiment'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b9d1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split  # Добавленный импорт\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Разделение на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Обучение модели логистической регрессии\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c03bb643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Word  Coefficient\n",
      "594         liked     0.200839\n",
      "177        cinema     0.199699\n",
      "477           her     0.156731\n",
      "676           mom     0.152476\n",
      "1116           tv     0.151803\n",
      "...           ...          ...\n",
      "1028         such    -0.173264\n",
      "540          jake    -0.182749\n",
      "409         funny    -0.185871\n",
      "951          show    -0.210488\n",
      "939   shakespeare    -0.217471\n",
      "\n",
      "[1222 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Исследование коэффициентов модели\n",
    "coefficients = model.coef_[0]\n",
    "feature_importance = pd.DataFrame({'Word': feature_names, 'Coefficient': coefficients})\n",
    "feature_importance = feature_importance.sort_values(by='Coefficient', ascending=False)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "351f8323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топики и соответствующие слова:\n",
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "mattei        being         jake          boll          show          \n",
      "oddness       exactly       drama         three         almost        \n",
      "these         use           parents       carver        towards       \n",
      "funny         called        terror        george        such          \n",
      "different     thats         version       constant      wrong         \n",
      "mr            watched       itself        fantastic     hal           \n",
      "people        play          zombie        greetings     die           \n",
      "encounter     look          closet        bart          funny         \n",
      "next          re            thriller      prisoners     laughs        \n",
      "relations     person        came          roll          full          \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Теперь выполним моделирование тем на том же наборе данных\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method='batch', max_iter=10, random_state=0)\n",
    "document_topics = lda.fit_transform(X_tfidf)\n",
    "\n",
    "# Вывод результатов моделирования тем\n",
    "print(\"Топики и соответствующие слова:\")\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Выводим топовые слова для каждой темы\n",
    "mglearn.tools.print_topics(topics=range(5), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10770521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "mattei        being         jake          boll          show          \n",
      "oddness       exactly       drama         three         almost        \n",
      "these         use           parents       carver        towards       \n",
      "funny         called        terror        george        such          \n",
      "different     thats         version       constant      wrong         \n",
      "mr            watched       itself        fantastic     hal           \n",
      "people        play          zombie        greetings     die           \n",
      "encounter     look          closet        bart          funny         \n",
      "next          re            thriller      prisoners     laughs        \n",
      "relations     person        came          roll          full          \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "oz            cinema        amount        tv            shakespeare   \n",
      "her           newbury       halliwell     camp          keitel        \n",
      "fun           tigers        realism       hell          write         \n",
      "kids          now           production    laughter      played        \n",
      "violence      75            particularly  gut           something     \n",
      "may           shot          our           wrenching     prevents      \n",
      "show          dan           piece         mom           forward       \n",
      "woody         appearance    hrs           liked         positive      \n",
      "thought       fitness       isn           artist        less          \n",
      "years         shame         takes         young         extreme       \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Сортировка тем по важности в порядке убывания\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "\n",
    "# Получение имен признаков из векторизатора\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Вывод топ-10 слов для каждой темы\n",
    "mglearn.tools.print_topics(topics=range(10), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cecf477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "mattei        being         jake          boll          show          \n",
      "oddness       exactly       drama         three         almost        \n",
      "these         use           parents       carver        towards       \n",
      "funny         called        terror        george        such          \n",
      "different     thats         version       constant      wrong         \n",
      "mr            watched       itself        fantastic     hal           \n",
      "people        play          zombie        greetings     die           \n",
      "encounter     look          closet        bart          funny         \n",
      "next          re            thriller      prisoners     laughs        \n",
      "relations     person        came          roll          full          \n",
      "each          again         formula       clooney       believe       \n",
      "currently     kill          horror        recommand     painful       \n",
      "eventually    here          12            sorrow        especially    \n",
      "humour        michael       year          fan           bit           \n",
      "imagine       how           monster       cry           how           \n",
      "interest      action        these         man           air           \n",
      "actual        point         neeson        famous        original      \n",
      "budget        making        attempts      everybody     band          \n",
      "low           let           holds         soundtrack    entertaining  \n",
      "stoner        best          special       wanna         cliffhanger   \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "oz            cinema        amount        tv            shakespeare   \n",
      "her           newbury       halliwell     camp          keitel        \n",
      "fun           tigers        realism       hell          write         \n",
      "kids          now           production    laughter      played        \n",
      "violence      75            particularly  gut           something     \n",
      "may           shot          our           wrenching     prevents      \n",
      "show          dan           piece         mom           forward       \n",
      "woody         appearance    hrs           liked         positive      \n",
      "thought       fitness       isn           artist        less          \n",
      "years         shame         takes         young         extreme       \n",
      "fact          dies          although      original      rarely        \n",
      "15            haggery       absolutely    wasn          950           \n",
      "ll            dark          rajnikanth    machine       harvey        \n",
      "prison        berkshire     carries       taken         mistake       \n",
      "comedy        74            won           bizarre       lame          \n",
      "while         others        start         seahunt       four          \n",
      "boring        knows         music         sea           tune          \n",
      "forget        grizzly       anything      hunt          score         \n",
      "dialogue      sister        grow          why           song          \n",
      "hilarious     places        heard         children      obsessives    \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Если вы хотитим вывести топ-20 слов для выбранных тем\n",
    "topics = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])  # построили индексы тем по необходимости\n",
    "mglearn.tools.print_topics(topics=topics, feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d754376",
   "metadata": {},
   "source": [
    "Этот код выполняет сортировку тем, получение имен признаков и вывод топ-слов для каждой темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16c534a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch\n",
      " Mr\n",
      "\n",
      "Phil the Alien is one of those quirky films where the humour is based around the oddness of everything rather than actual punchlines\n",
      "At first it was very odd and pretty funny but as the movie progressed I didn't find the jokes or oddness funny anymore\n",
      "\n",
      "If you like original gut wrenching laughter you will like this movie\n",
      " If you are young or old then you will love this movie, hell even my mom liked it\n",
      "\n",
      "This a fantastic movie of three prisoners who become famous\n",
      " One of the actors is george clooney and I'm not a fan but this roll is not bad\n",
      "\n",
      "What an absolutely stunning movie, if you have 2\n",
      "5 hrs to kill, watch it, you won't regret it, it's too much fun! Rajnikanth carries the movie on his shoulders and although there isn't anything more other than him, I still liked it\n",
      "\n",
      "The cast played Shakespeare\n",
      "Shakespeare lost\n",
      "\n",
      "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time\n",
      "This movie is slower than a soap opera\n",
      "\n",
      "Some films just simply should not be remade\n",
      " This is one of them\n",
      "\n",
      "I remember this film,it was the first film i had watched at the cinema the picture was dark in places i was very nervous it was back in 74/75 my Dad took me my brother & sister to Newbury cinema in Newbury Berkshire England\n",
      " I recall the tigers and the lots of snow in the film also the appearance of Grizzly Adams actor Dan Haggery i think one of the tigers gets shot and dies\n",
      "\n",
      "Encouraged by the positive comments about this film on here I was looking forward to watching this film\n",
      " Bad mistake\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Вывести топ-10 документов, в которых указанная тема наиболее важна\n",
    "selected_topic = 0  # Подстройте индекс темы по необходимости\n",
    "document_indices = np.argsort(document_topics[:, selected_topic])[::-1]\n",
    "for doc_index in document_indices[:10]:\n",
    "    # Вывести первые два предложения\n",
    "    print('\\n'.join(cleaned_reviews[doc_index].split('.')[:2]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af8d2d",
   "metadata": {},
   "source": [
    "В ходе выполнения этой лабораторной работы мы освоили несколько важных моментов в анализе текстовых данных и построении моделей машинного обучения: \n",
    "Очистка текстовых данных от HTML-тегов с использованием библиотеки BeautifulSoup, а также преобразование текстов в представление \"мешка слов\" и TF-IDF. Применение метода Latent Dirichlet Allocation (LDA) для моделирования тем в корпусе текстов. Вывод топ-10 или топ-20 слов для каждой темы. Использование логистической регрессии для анализа сентимента в текстах. Обучение модели на TF-IDF представлении данных и анализ важности слов для сентимента. Импорт и использование различных инструментов из библиотеки scikit-learn, таких как CountVectorizer, TfidfTransformer, LatentDirichletAllocation, LogisticRegression, train_test_split и другие. \n",
    "Эта лабораторная работа дает представление о том, как проводить анализ текстовых данных, выделять темы в корпусе текстов, а также строить модели для задач анализа сентимента. Комбинация методов машинного обучения и анализа текстовых данных является мощным инструментом для извлечения знаний из текстов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
